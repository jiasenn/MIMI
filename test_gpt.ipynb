{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer to this for installation https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (0.8.56)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (1.4.50)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.5.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (2023.10.0)\n",
      "Requirement already satisfied: langchain>=0.0.303 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.0.327)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (1.26.1)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.28.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (2.1.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama_index) (1.26.18)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama_index) (3.20.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from deprecated>=1.2.9.3->llama_index) (1.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (0.0.54)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from langchain>=0.0.303->llama_index) (2.31.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from pandas->llama_index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.303->llama_index) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.303->llama_index) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.303->llama_index) (1.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama_index) (2.4)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama_index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.303->llama_index) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from click->nltk<4.0.0,>=3.8.1->llama_index) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (0.2.11)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama-cpp-python) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama-cpp-python) (1.26.1)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pypdf in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\downloads\\github\\mimi\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install dependencies\n",
    "%pip install llama_index\n",
    "%pip install llama-cpp-python\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pypdf\n",
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    \n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    \n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    \n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,  # note, this sets n_ctx in the model_kwargs below, so you don't need to pass it there.\n",
    "    \n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    \n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 30}, # I need to play with this and see if it actually helps\n",
    "    \n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.101956 seconds\n",
      "      |_chunking ->  0.003991 seconds\n",
      "      |_chunking ->  0.006986 seconds\n",
      "      |_chunking ->  0.00199 seconds\n",
      "      |_chunking ->  0.001999 seconds\n",
      "      |_chunking ->  0.000994 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.000993 seconds\n",
      "      |_chunking ->  0.001999 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.001994 seconds\n",
      "      |_chunking ->  0.000995 seconds\n",
      "      |_chunking ->  0.001001 seconds\n",
      "      |_chunking ->  0.000994 seconds\n",
      "      |_chunking ->  0.001 seconds\n",
      "      |_chunking ->  0.005984 seconds\n",
      "      |_chunking ->  0.005981 seconds\n",
      "      |_chunking ->  0.004989 seconds\n",
      "      |_chunking ->  0.004987 seconds\n",
      "      |_chunking ->  0.006978 seconds\n",
      "      |_chunking ->  0.005987 seconds\n",
      "      |_chunking ->  0.000994 seconds\n",
      "      |_chunking ->  0.0 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.001001 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.000998 seconds\n",
      "      |_chunking ->  0.004983 seconds\n",
      "      |_chunking ->  0.000994 seconds\n",
      "      |_chunking ->  0.001001 seconds\n",
      "      |_chunking ->  0.003989 seconds\n",
      "      |_chunking ->  0.000994 seconds\n",
      "      |_chunking ->  0.001 seconds\n",
      "      |_chunking ->  0.000997 seconds\n",
      "      |_chunking ->  0.0 seconds\n",
      "    |_embedding ->  0.251061 seconds\n",
      "    |_embedding ->  0.100884 seconds\n",
      "    |_embedding ->  0.095131 seconds\n",
      "    |_embedding ->  0.096908 seconds\n",
      "    |_embedding ->  0.031885 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x275750a6d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an index of your documents\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "storage_directory = \"./storage\"\n",
    "\n",
    "documents = SimpleDirectoryReader('./testdata').load_data()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024,\n",
    "                                               embed_model=\"local\",\n",
    "                                               callback_manager=callback_manager)\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "# Persist the index to disk\n",
    "display(index)\n",
    "index.storage_context.persist(persist_dir=storage_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# Now you can load the index from disk when needed, and not rebuild it each time.\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# transcript_directory = \"transcripts/ancient-aliens-official\"\n",
    "storage_directory = \"./storage\"\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024,\n",
    "                                               embed_model=\"local\",\n",
    "                                               callback_manager=callback_manager)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_directory)\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  317.511931 seconds\n",
      "      |_retrieve ->  0.030923 seconds\n",
      "        |_embedding ->  0.021941 seconds\n",
      "      |_synthesize ->  317.481008 seconds\n",
      "        |_templating ->  0.0 seconds\n",
      "        |_llm ->  317.458696 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>  Based on your quiz results and choices, your personality is best described as a \"Community Mentor\" because you prioritize helping others and contributing to the community. Your archetype is the \"Kampong Leader\" because you value community bonding and mentorship, and you are willing to invest time and resources to support those in need.\n",
       "\n",
       "Your results indicate that you resonate with the values of a traditional Singaporean community leader, who prioritizes the well-being of others and works towards the betterment of the community as a whole. You are likely someone who is approachable, empathetic, and willing to listen to others, and you may have a strong sense of responsibility towards your community.\n",
       "\n",
       "In terms of your personality, you seem to be a caring and compassionate individual who values relationships and community over material possessions or personal gain. You are likely someone who is patient, understanding, and willing to lend a helping hand when needed. Your archetype reflects these qualities, as the Kampong Leader is known for their wisdom, kindness, and dedication to their community.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query your index!\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=3)\n",
    "\n",
    "query = \"Given the quiz from quiz.txt and the results from choices.txt describe my personality and the archetype from the information in all the files provided. \\n\" \\\n",
    "        \"Please use the following format: \\n\" \\\n",
    "        \"Your personality is <personality> because <reasoning>. \\n\" \\\n",
    "        \"Your archetype is <archetype> because <reasoning>. \\n\" \\\n",
    "        \"Personality should not be aan archetype.\"\n",
    "\n",
    "        # \"Please summarize your reasoning into 3 sentences. \\n\" \\\n",
    "\n",
    "\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Downloads\\Github\\MIMI\\test_gpt.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Downloads/Github/MIMI/test_gpt.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mStop here\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  474.15284 seconds\n",
      "      |_retrieve ->  0.553662 seconds\n",
      "        |_embedding ->  0.492307 seconds\n",
      "      |_synthesize ->  473.596518 seconds\n",
      "        |_templating ->  0.0 seconds\n",
      "        |_llm ->  473.51626 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>  Based on the given context, I would rather be a Samsui woman than live in a stressful society in Singapore's context. As a Samsui woman, I would have a strong support system and a sense of community, while also having the opportunity to contribute to the development of Singapore through my hard work and resilience. In contrast, living in a stressful society in Singapore's context may lead to burnout and a lack of fulfillment, despite the potential for higher wages and better living conditions.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Would you rather be a Samsui women or live in such a stressful society in Singapore's context \\n\" \\\n",
    "        \"Please summarize your reasoning into 2 sentences.\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  38.681917 seconds\n",
      "      |_retrieve ->  0.184877 seconds\n",
      "        |_embedding ->  0.165846 seconds\n",
      "      |_synthesize ->  38.49704 seconds\n",
      "        |_templating ->  0.0 seconds\n",
      "        |_llm ->  38.479332 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>  Based on your responses to the quiz questions, you seem to possess a combination of traits from different archetypes. Your desire to contribute to the community and help others (Question 1), your preference for walking or eco-friendly transportation (Question 2), and your interest in cultural events and education (Question 3) suggest that you resonate with the Samsui Woman archetype. Additionally, your willingness to volunteer and teach (Question 4) and your focus on community bonding and mentorship (Question 5) further support this conclusion.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Given the quiz from quiz.txt and the choices from choices.txt describe my personality and which archetype I'm most probably will be. \\n\" \\\n",
    "        \"Please summarize your reasoning into 2 sentences.\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
